{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XFg91xOYSXf",
        "outputId": "a07331e4-5fcb-4faa-e095-0e872277785f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: Typing in /usr/local/lib/python3.11/dist-packages (3.7.4.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install pygame\n",
        "!pip install numpy\n",
        "!pip install Typing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pygame\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "from typing import Tuple\n",
        "\n",
        "GRID_WIDTH, GRID_HEIGHT = 10, 20\n",
        "CELL_SIZE = 30\n",
        "SCREEN_WIDTH = GRID_WIDTH * CELL_SIZE\n",
        "SCREEN_HEIGHT = GRID_HEIGHT * CELL_SIZE + 100\n",
        "\n",
        "COLORS = {\n",
        "    0: (0, 0, 0),\n",
        "    1: (0, 240, 240),\n",
        "    2: (240, 240, 0),\n",
        "    3: (160, 0, 240),\n",
        "    4: (240, 0, 0),\n",
        "    5: (0, 240, 0),\n",
        "    'grid': (40, 40, 40)\n",
        "}\n",
        "\n",
        "SHAPES = [\n",
        "    (np.array([[1, 1, 1, 1]]), 1),\n",
        "    (np.array([[2, 2], [2, 2]]), 2),\n",
        "    (np.array([[0, 3, 0], [3, 3, 3]]), 3),\n",
        "    (np.array([[4, 4, 0], [0, 4, 4]]), 4),\n",
        "    (np.array([[0, 5, 5], [5, 5, 0]]), 5),\n",
        "]\n",
        "\n",
        "class Tetromino:\n",
        "    def __init__(self, shape: np.ndarray, color_index: int):\n",
        "        self.shape = shape\n",
        "        self.color_index = color_index\n",
        "        self.x = GRID_WIDTH // 2 - shape.shape[1] // 2\n",
        "        self.y = 0\n",
        "\n",
        "    def rotate(self) -> np.ndarray:\n",
        "        return np.rot90(self.shape)\n",
        "\n",
        "class TetrisGame:\n",
        "    def __init__(self):\n",
        "        pygame.init()\n",
        "        self.screen = pygame.display.set_mode((SCREEN_WIDTH, SCREEN_HEIGHT))\n",
        "        pygame.display.set_caption(\"Tetris\")\n",
        "        self.clock = pygame.time.Clock()\n",
        "        self.font = pygame.font.Font(None, 36)\n",
        "        self.fall_time = 0\n",
        "        self.fall_speed = 1000\n",
        "        self.score = 0\n",
        "        self.level = 1\n",
        "        self.lines_cleared = 0\n",
        "        self.line_scores = {1: 100, 2: 300, 3: 500, 4: 800}\n",
        "        self.reset()\n",
        "\n",
        "    def _new_piece(self) -> 'Tetromino':\n",
        "        shape, color = random.choice(SHAPES)\n",
        "        return Tetromino(shape, color)\n",
        "\n",
        "    def _check_collision(self, shape: np.ndarray, x: int, y: int) -> bool:\n",
        "        for row in range(shape.shape[0]):\n",
        "            for col in range(shape.shape[1]):\n",
        "                if shape[row][col] != 0:\n",
        "                    grid_x = x + col\n",
        "                    grid_y = y + row\n",
        "                    if (grid_x < 0 or grid_x >= GRID_WIDTH or\n",
        "                        grid_y >= GRID_HEIGHT or\n",
        "                        (grid_y >= 0 and self.grid[grid_y][grid_x] != 0)):\n",
        "                        return True\n",
        "        return False\n",
        "\n",
        "    def _merge_piece(self):\n",
        "        for row in range(self.current_piece.shape.shape[0]):\n",
        "            for col in range(self.current_piece.shape.shape[1]):\n",
        "                if self.current_piece.shape[row][col] != 0:\n",
        "                    x = self.current_piece.x + col\n",
        "                    y = self.current_piece.y + row\n",
        "                    if 0 <= y < GRID_HEIGHT:\n",
        "                        self.grid[y][x] = self.current_piece.color_index\n",
        "\n",
        "    def _clear_lines(self):\n",
        "        complete_lines = 0\n",
        "        new_grid = []\n",
        "        for row in self.grid:\n",
        "            if not np.all(row != 0):\n",
        "                new_grid.append(row)\n",
        "            else:\n",
        "                complete_lines += 1\n",
        "        while len(new_grid) < GRID_HEIGHT:\n",
        "            new_grid.insert(0, np.zeros(GRID_WIDTH, dtype=int))\n",
        "        self.grid = np.array(new_grid)\n",
        "\n",
        "        if complete_lines > 0:\n",
        "            self.lines_cleared += complete_lines\n",
        "            self.score += self.line_scores.get(complete_lines, 0)\n",
        "            self.level = self.lines_cleared // 10 + 1\n",
        "\n",
        "        return complete_lines\n",
        "\n",
        "    def step(self, action: int) -> Tuple[np.ndarray, int, bool]:\n",
        "        moved = False\n",
        "        if action == 0 and not self._check_collision(self.current_piece.shape, self.current_piece.x - 1, self.current_piece.y):\n",
        "            self.current_piece.x -= 1\n",
        "            moved = True\n",
        "        elif action == 1 and not self._check_collision(self.current_piece.shape, self.current_piece.x + 1, self.current_piece.y):\n",
        "            self.current_piece.x += 1\n",
        "            moved = True\n",
        "        elif action == 2:\n",
        "            rotated_shape = self.current_piece.rotate()\n",
        "            if not self._check_collision(rotated_shape, self.current_piece.x, self.current_piece.y):\n",
        "                self.current_piece.shape = rotated_shape\n",
        "                moved = True\n",
        "        elif action == 3:\n",
        "            while not self._check_collision(self.current_piece.shape, self.current_piece.x, self.current_piece.y + 1):\n",
        "                self.current_piece.y += 1\n",
        "            self._merge_piece()\n",
        "            lines_cleared = self._clear_lines()\n",
        "            reward = lines_cleared * 100 + (self.current_piece.y * 2)\n",
        "            self.current_piece = self._new_piece()\n",
        "            if self._check_collision(self.current_piece.shape, self.current_piece.x, self.current_piece.y):\n",
        "                self.game_over = True\n",
        "            return self.get_state(), reward, self.game_over\n",
        "        return self.get_state(), 1 if moved else -1, self.game_over\n",
        "\n",
        "    def reset(self) -> np.ndarray:\n",
        "        self.grid = np.zeros((GRID_HEIGHT, GRID_WIDTH), dtype=int)\n",
        "        self.current_piece = self._new_piece()\n",
        "        self.game_over = False\n",
        "        self.score = 0\n",
        "        self.lines_cleared = 0\n",
        "        self.level = 1\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self) -> np.ndarray:\n",
        "        state = self.grid.copy()\n",
        "        for row in range(self.current_piece.shape.shape[0]):\n",
        "            for col in range(self.current_piece.shape.shape[1]):\n",
        "                if self.current_piece.shape[row][col] != 0:\n",
        "                    x = self.current_piece.x + col\n",
        "                    y = self.current_piece.y + row\n",
        "                    if 0 <= y < GRID_HEIGHT:\n",
        "                        state[y][x] = self.current_piece.color_index\n",
        "        return state\n",
        "\n",
        "    def update(self, dt: int):\n",
        "        self.fall_time += dt\n",
        "        if self.fall_time > self.fall_speed / self.level:\n",
        "            self.fall_time = 0\n",
        "            if not self._check_collision(self.current_piece.shape, self.current_piece.x, self.current_piece.y + 1):\n",
        "                self.current_piece.y += 1\n",
        "            else:\n",
        "                self._merge_piece()\n",
        "                lines_cleared = self._clear_lines()\n",
        "                self.current_piece = self._new_piece()\n",
        "                if self._check_collision(self.current_piece.shape, self.current_piece.x, self.current_piece.y):\n",
        "                    self.game_over = True\n",
        "\n",
        "    def render(self):\n",
        "        self.screen.fill((0, 0, 0))\n",
        "\n",
        "        # Draw grid background\n",
        "        for x in range(GRID_WIDTH):\n",
        "            for y in range(GRID_HEIGHT):\n",
        "                rect = pygame.Rect(x * CELL_SIZE, y * CELL_SIZE, CELL_SIZE, CELL_SIZE)\n",
        "                pygame.draw.rect(self.screen, COLORS['grid'], rect, 1)\n",
        "\n",
        "        # Draw placed pieces\n",
        "        for y in range(GRID_HEIGHT):\n",
        "            for x in range(GRID_WIDTH):\n",
        "                if self.grid[y][x] != 0:\n",
        "                    rect = pygame.Rect(x * CELL_SIZE, y * CELL_SIZE, CELL_SIZE, CELL_SIZE)\n",
        "                    pygame.draw.rect(self.screen, COLORS[self.grid[y][x]], rect)\n",
        "\n",
        "        # Draw current piece\n",
        "        for row in range(self.current_piece.shape.shape[0]):\n",
        "            for col in range(self.current_piece.shape.shape[1]):\n",
        "                if self.current_piece.shape[row][col] != 0:\n",
        "                    x = (self.current_piece.x + col) * CELL_SIZE\n",
        "                    y = (self.current_piece.y + row) * CELL_SIZE\n",
        "                    rect = pygame.Rect(x, y, CELL_SIZE, CELL_SIZE)\n",
        "                    pygame.draw.rect(self.screen, COLORS[self.current_piece.color_index], rect)\n",
        "\n",
        "        # Render score and level\n",
        "        score_text = self.font.render(f'Score: {self.score}', True, (255, 255, 255))\n",
        "        level_text = self.font.render(f'Level: {self.level}', True, (255, 255, 255))\n",
        "\n",
        "        # Render texts below the grid\n",
        "        self.screen.blit(score_text, (10, SCREEN_HEIGHT - 80))\n",
        "        self.screen.blit(level_text, (10, SCREEN_HEIGHT - 40))\n",
        "\n",
        "        pygame.display.flip()\n",
        "\n",
        "def main():\n",
        "    game = TetrisGame()\n",
        "    running = True\n",
        "    while running:\n",
        "        dt = game.clock.tick(60)\n",
        "        for event in pygame.event.get():\n",
        "            if event.type == pygame.QUIT:\n",
        "                running = False\n",
        "            if event.type == pygame.KEYDOWN:\n",
        "                if event.key == pygame.K_LEFT:\n",
        "                    game.step(0)\n",
        "                elif event.key == pygame.K_RIGHT:\n",
        "                    game.step(1)\n",
        "                elif event.key == pygame.K_UP:\n",
        "                    game.step(2)\n",
        "                elif event.key == pygame.K_DOWN:\n",
        "                    game.step(3)\n",
        "\n",
        "        game.update(dt)\n",
        "        game.render()\n",
        "\n",
        "        if game.game_over:\n",
        "            game_over_text = game.font.render('GAME OVER', True, (255, 0, 0))\n",
        "            game.screen.blit(game_over_text, (SCREEN_WIDTH // 2 - game_over_text.get_width() // 2, SCREEN_HEIGHT // 2))\n",
        "            pygame.display.flip()\n",
        "            pygame.time.wait(2000)\n",
        "            running = False\n",
        "\n",
        "    pygame.quit()\n",
        "    sys.exit()"
      ],
      "metadata": {
        "id": "bMEwXQ6vY3EF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4d0c86c-5a3a-43fb-8613-e54998b71b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 2.6.1 (SDL 2.28.4, Python 3.11.11)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import pygame\n",
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(list(self.buffer), batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class TetrisCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TetrisCNN, self).__init__()\n",
        "        # Fully connected layers for grid-based state\n",
        "        input_size = GRID_WIDTH * GRID_HEIGHT\n",
        "        hidden_size = 256\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, 4)  # 4 actions: left, right, rotate, drop\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input state\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "class TetrisAI:\n",
        "    def __init__(self, game):\n",
        "        self.game = game\n",
        "        self.model = TetrisCNN()\n",
        "        self.target_model = TetrisCNN()\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        self.replay_buffer = ReplayBuffer(10000)\n",
        "\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.999\n",
        "        self.gamma = 0.99\n",
        "        self.batch_size = 32\n",
        "        self.target_update = 10\n",
        "        self.steps = 0\n",
        "\n",
        "    def get_state_tensor(self, state):\n",
        "        # Convert numpy array to torch tensor\n",
        "        return torch.FloatTensor(state).unsqueeze(0)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, 3)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            state_tensor = self.get_state_tensor(state)\n",
        "            q_values = self.model(state_tensor)\n",
        "            return q_values.argmax().item()\n",
        "\n",
        "    def train_step(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        transitions = self.replay_buffer.sample(self.batch_size)\n",
        "        batch = list(zip(*transitions))\n",
        "\n",
        "        state_batch = torch.FloatTensor(batch[0])\n",
        "        action_batch = torch.LongTensor(batch[1]).unsqueeze(1)\n",
        "        reward_batch = torch.FloatTensor(batch[2]).unsqueeze(1)\n",
        "        next_state_batch = torch.FloatTensor(batch[3])\n",
        "        done_batch = torch.FloatTensor(batch[4]).unsqueeze(1)\n",
        "\n",
        "        current_q_values = self.model(state_batch).gather(1, action_batch)\n",
        "        next_q_values = self.target_model(next_state_batch).max(1)[0].detach().unsqueeze(1)\n",
        "        target_q_values = reward_batch + (1 - done_batch) * self.gamma * next_q_values\n",
        "\n",
        "        loss = F.smooth_l1_loss(current_q_values, target_q_values)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.steps += 1\n",
        "        if self.steps % self.target_update == 0:\n",
        "            self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def train(self, num_episodes):\n",
        "        for episode in range(num_episodes):\n",
        "            state = self.game.reset()\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                action = self.select_action(state)\n",
        "                next_state, reward, done = self.game.step(action)\n",
        "\n",
        "                self.replay_buffer.push(state, action, reward, next_state, done)\n",
        "                self.train_step()\n",
        "\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "\n",
        "            # Decay epsilon\n",
        "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "            if episode % 10 == 0:\n",
        "                print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {self.epsilon:.3f}\")\n",
        "\n",
        "def main():\n",
        "    game = TetrisGame()\n",
        "    ai = TetrisAI(game)\n",
        "    ai.train(num_episodes=1000)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdscF5SDZCMi",
        "outputId": "001de66c-ed39-4da3-b1db-0f28c4febe4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-d0eb15466327>:77: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  state_batch = torch.FloatTensor(batch[0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward: 292, Epsilon: 0.999\n",
            "Episode 10, Total Reward: 286, Epsilon: 0.989\n",
            "Episode 20, Total Reward: 248, Epsilon: 0.979\n",
            "Episode 30, Total Reward: 263, Epsilon: 0.969\n",
            "Episode 40, Total Reward: 333, Epsilon: 0.960\n",
            "Episode 50, Total Reward: 187, Epsilon: 0.950\n",
            "Episode 60, Total Reward: 279, Epsilon: 0.941\n",
            "Episode 70, Total Reward: 397, Epsilon: 0.931\n",
            "Episode 80, Total Reward: 247, Epsilon: 0.922\n",
            "Episode 90, Total Reward: 275, Epsilon: 0.913\n",
            "Episode 100, Total Reward: 418, Epsilon: 0.904\n",
            "Episode 110, Total Reward: 261, Epsilon: 0.895\n",
            "Episode 120, Total Reward: 286, Epsilon: 0.886\n",
            "Episode 130, Total Reward: 350, Epsilon: 0.877\n",
            "Episode 140, Total Reward: 329, Epsilon: 0.868\n",
            "Episode 150, Total Reward: 292, Epsilon: 0.860\n",
            "Episode 160, Total Reward: 199, Epsilon: 0.851\n",
            "Episode 170, Total Reward: 314, Epsilon: 0.843\n",
            "Episode 180, Total Reward: 258, Epsilon: 0.834\n",
            "Episode 190, Total Reward: 260, Epsilon: 0.826\n",
            "Episode 200, Total Reward: 227, Epsilon: 0.818\n",
            "Episode 210, Total Reward: 490, Epsilon: 0.810\n",
            "Episode 220, Total Reward: 422, Epsilon: 0.802\n",
            "Episode 230, Total Reward: 299, Epsilon: 0.794\n",
            "Episode 240, Total Reward: 337, Epsilon: 0.786\n",
            "Episode 250, Total Reward: 222, Epsilon: 0.778\n",
            "Episode 260, Total Reward: 259, Epsilon: 0.770\n",
            "Episode 270, Total Reward: 367, Epsilon: 0.763\n",
            "Episode 280, Total Reward: 232, Epsilon: 0.755\n",
            "Episode 290, Total Reward: 347, Epsilon: 0.747\n",
            "Episode 300, Total Reward: 461, Epsilon: 0.740\n",
            "Episode 310, Total Reward: 346, Epsilon: 0.733\n",
            "Episode 320, Total Reward: 314, Epsilon: 0.725\n",
            "Episode 330, Total Reward: 270, Epsilon: 0.718\n",
            "Episode 340, Total Reward: 272, Epsilon: 0.711\n",
            "Episode 350, Total Reward: 349, Epsilon: 0.704\n",
            "Episode 360, Total Reward: 354, Epsilon: 0.697\n",
            "Episode 370, Total Reward: 342, Epsilon: 0.690\n",
            "Episode 380, Total Reward: 433, Epsilon: 0.683\n",
            "Episode 390, Total Reward: 530, Epsilon: 0.676\n",
            "Episode 400, Total Reward: 430, Epsilon: 0.670\n",
            "Episode 410, Total Reward: 423, Epsilon: 0.663\n",
            "Episode 420, Total Reward: 395, Epsilon: 0.656\n",
            "Episode 430, Total Reward: 427, Epsilon: 0.650\n",
            "Episode 440, Total Reward: 462, Epsilon: 0.643\n",
            "Episode 450, Total Reward: 296, Epsilon: 0.637\n",
            "Episode 460, Total Reward: 287, Epsilon: 0.631\n",
            "Episode 470, Total Reward: 381, Epsilon: 0.624\n",
            "Episode 480, Total Reward: 379, Epsilon: 0.618\n",
            "Episode 490, Total Reward: 471, Epsilon: 0.612\n",
            "Episode 500, Total Reward: 310, Epsilon: 0.606\n",
            "Episode 510, Total Reward: 401, Epsilon: 0.600\n",
            "Episode 520, Total Reward: 446, Epsilon: 0.594\n",
            "Episode 530, Total Reward: 485, Epsilon: 0.588\n",
            "Episode 540, Total Reward: 368, Epsilon: 0.582\n",
            "Episode 550, Total Reward: 367, Epsilon: 0.576\n",
            "Episode 560, Total Reward: 492, Epsilon: 0.570\n",
            "Episode 570, Total Reward: 405, Epsilon: 0.565\n",
            "Episode 580, Total Reward: 280, Epsilon: 0.559\n",
            "Episode 590, Total Reward: 355, Epsilon: 0.554\n",
            "Episode 600, Total Reward: 428, Epsilon: 0.548\n",
            "Episode 610, Total Reward: 324, Epsilon: 0.543\n",
            "Episode 620, Total Reward: 498, Epsilon: 0.537\n",
            "Episode 630, Total Reward: 242, Epsilon: 0.532\n",
            "Episode 640, Total Reward: 509, Epsilon: 0.527\n",
            "Episode 650, Total Reward: 382, Epsilon: 0.521\n",
            "Episode 660, Total Reward: 394, Epsilon: 0.516\n",
            "Episode 670, Total Reward: 363, Epsilon: 0.511\n",
            "Episode 680, Total Reward: 295, Epsilon: 0.506\n",
            "Episode 690, Total Reward: 444, Epsilon: 0.501\n",
            "Episode 700, Total Reward: 328, Epsilon: 0.496\n",
            "Episode 710, Total Reward: 519, Epsilon: 0.491\n",
            "Episode 720, Total Reward: 332, Epsilon: 0.486\n",
            "Episode 730, Total Reward: 417, Epsilon: 0.481\n",
            "Episode 740, Total Reward: 341, Epsilon: 0.476\n",
            "Episode 750, Total Reward: 427, Epsilon: 0.472\n",
            "Episode 760, Total Reward: 432, Epsilon: 0.467\n",
            "Episode 770, Total Reward: 392, Epsilon: 0.462\n",
            "Episode 780, Total Reward: 436, Epsilon: 0.458\n",
            "Episode 790, Total Reward: 479, Epsilon: 0.453\n",
            "Episode 800, Total Reward: 350, Epsilon: 0.449\n",
            "Episode 810, Total Reward: 371, Epsilon: 0.444\n",
            "Episode 820, Total Reward: 576, Epsilon: 0.440\n",
            "Episode 830, Total Reward: 412, Epsilon: 0.435\n",
            "Episode 840, Total Reward: 399, Epsilon: 0.431\n",
            "Episode 850, Total Reward: 382, Epsilon: 0.427\n",
            "Episode 860, Total Reward: 421, Epsilon: 0.423\n",
            "Episode 870, Total Reward: 510, Epsilon: 0.418\n",
            "Episode 880, Total Reward: 434, Epsilon: 0.414\n",
            "Episode 890, Total Reward: 255, Epsilon: 0.410\n",
            "Episode 900, Total Reward: 446, Epsilon: 0.406\n",
            "Episode 910, Total Reward: 383, Epsilon: 0.402\n",
            "Episode 920, Total Reward: 296, Epsilon: 0.398\n",
            "Episode 930, Total Reward: 373, Epsilon: 0.394\n",
            "Episode 940, Total Reward: 316, Epsilon: 0.390\n",
            "Episode 950, Total Reward: 502, Epsilon: 0.386\n",
            "Episode 960, Total Reward: 367, Epsilon: 0.382\n",
            "Episode 970, Total Reward: 265, Epsilon: 0.379\n",
            "Episode 980, Total Reward: 402, Epsilon: 0.375\n",
            "Episode 990, Total Reward: 313, Epsilon: 0.371\n"
          ]
        }
      ]
    }
  ]
}